{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {"display_name": "Python 3","language": "python","name": "python3"},
  "language_info": {"name": "python","version": "3.10.0"}
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["# English Verb Phonetics — NLP & Data Science Analysis\n\n**Goal:** Analyze 300+ English verbs to find patterns in how past tenses are formed and pronounced.\n\nWe will:\n1. Load and explore the dataset\n2. Visualize phonetic patterns\n3. Engineer features from text\n4. Train a classifier: regular vs. irregular verb\n5. Evaluate and explain the model\n\n---"]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["## Step 0 — Setup: Import Libraries\n\nThese are the tools we need. Think of them as your toolbox."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data handling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import seaborn as sns\n",
    "\n",
    "# Machine learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    classification_report, confusion_matrix,\n",
    "    accuracy_score, ConfusionMatrixDisplay\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# Style\n",
    "sns.set_theme(style='whitegrid', palette='muted')\n",
    "plt.rcParams['figure.dpi'] = 120\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "\n",
    "print('All libraries loaded successfully!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## Step 1 — Load the Dataset"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both sheets from the Excel file\n",
    "# Make sure english_verbs.xlsx is in the same folder, or adjust the path\n",
    "df_reg   = pd.read_excel('../data/english_verbs.xlsx', sheet_name='Regular Verbs',   header=2)\n",
    "df_irreg = pd.read_excel('../data/english_verbs.xlsx', sheet_name='Irregular Verbs', header=2)\n",
    "\n",
    "# Rename columns for easier use in code\n",
    "reg_cols = ['Base','Simple_Past','Past_Participle',\n",
    "            'IPA_Base','IPA_Past','IPA_PP',\n",
    "            'Phonetic_Base','Phonetic_Past','Phonetic_PP',\n",
    "            'Last_Sound','Ending']\n",
    "irreg_cols = ['Base','Simple_Past','Past_Participle',\n",
    "              'IPA_Base','IPA_Past','IPA_PP',\n",
    "              'Phonetic_Base','Phonetic_Past','Phonetic_PP',\n",
    "              'Vowel_Change']\n",
    "\n",
    "df_reg.columns   = reg_cols\n",
    "df_irreg.columns = irreg_cols\n",
    "\n",
    "# Drop empty rows if any\n",
    "df_reg   = df_reg.dropna(subset=['Base']).reset_index(drop=True)\n",
    "df_irreg = df_irreg.dropna(subset=['Base']).reset_index(drop=True)\n",
    "\n",
    "# Add a label column — this is our TARGET for machine learning\n",
    "df_reg['Type']   = 'Regular'\n",
    "df_irreg['Type'] = 'Irregular'\n",
    "\n",
    "print(f'Regular verbs loaded:   {len(df_reg)}')\n",
    "print(f'Irregular verbs loaded: {len(df_irreg)}')\n",
    "print(f'Total:                  {len(df_reg) + len(df_irreg)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Always start by looking at your data\n",
    "print('--- REGULAR VERBS (first 5) ---')\n",
    "display(df_reg.head())\n",
    "\n",
    "print('\\n--- IRREGULAR VERBS (first 5) ---')\n",
    "display(df_irreg.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## Step 2 — Exploratory Data Analysis (EDA)\n\nEDA means: **look at the data and find patterns before any machine learning.**\n\nThis is the most important step in data science. A good data scientist spends 60-70% of their time here."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2A — How many regular vs irregular verbs?\n",
    "fig, ax = plt.subplots(figsize=(7, 5))\n",
    "\n",
    "counts = {'Regular': len(df_reg), 'Irregular': len(df_irreg)}\n",
    "colors = ['#1D4E5A', '#4A1C2A']\n",
    "\n",
    "bars = ax.bar(counts.keys(), counts.values(), color=colors, width=0.5, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, val in zip(bars, counts.values()):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 2,\n",
    "            str(val), ha='center', va='bottom', fontweight='bold', fontsize=13)\n",
    "\n",
    "ax.set_title('Distribution: Regular vs Irregular Verbs', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_ylabel('Number of Verbs', fontsize=11)\n",
    "ax.set_ylim(0, max(counts.values()) * 1.15)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_01_distribution.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Insight: English has more regular verbs, but irregular verbs include the most common ones (be, have, go, do...)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2B — Regular verbs: how is -ed pronounced?\n",
    "# This tests the phonetic rule we built into the dataset\n",
    "\n",
    "ending_counts = df_reg['Ending'].value_counts()\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "# Bar chart\n",
    "colors_end = {'/ t/':'#2E7D52', '/d/':'#1A3A7A', '/\\u026ad/':'#8B6914'}\n",
    "bar_colors = [colors_end.get(e, '#888888') for e in ending_counts.index]\n",
    "axes[0].bar(ending_counts.index, ending_counts.values, color=['#2E7D52','#1A3A7A','#8B6914'],\n",
    "            edgecolor='white', linewidth=1.5)\n",
    "for i, (idx, val) in enumerate(ending_counts.items()):\n",
    "    axes[0].text(i, val + 1, str(val), ha='center', fontweight='bold', fontsize=12)\n",
    "axes[0].set_title('-ed Pronunciation: Count by Type', fontweight='bold', fontsize=13)\n",
    "axes[0].set_xlabel('Pronunciation of -ed', fontsize=11)\n",
    "axes[0].set_ylabel('Number of Verbs', fontsize=11)\n",
    "axes[0].spines['top'].set_visible(False)\n",
    "axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(ending_counts.values,\n",
    "            labels=[f'{idx}\\n({v} verbs)' for idx,v in ending_counts.items()],\n",
    "            colors=['#2E7D52','#1A3A7A','#8B6914'],\n",
    "            autopct='%1.1f%%', startangle=90,\n",
    "            textprops={'fontsize': 11},\n",
    "            wedgeprops={'edgecolor':'white','linewidth':2})\n",
    "axes[1].set_title('-ed Pronunciation: Proportions', fontweight='bold', fontsize=13)\n",
    "\n",
    "plt.suptitle('The Phonetic Rule for Regular Verb Past Tense', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_02_endings.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2C — What sounds trigger each ending?\n",
    "print('Voiceless consonants (→ /t/ ending):')\n",
    "display(df_reg[df_reg['Ending']=='/t/'][['Base','Last_Sound','Ending']].head(10))\n",
    "\n",
    "print('\\nVoiced sounds (→ /d/ ending):')\n",
    "display(df_reg[df_reg['Ending']=='/d/'][['Base','Last_Sound','Ending']].head(10))\n",
    "\n",
    "print('\\nT or D sounds (→ /ɪd/ ending):')\n",
    "display(df_reg[df_reg['Ending']=='/ɪd/'][['Base','Last_Sound','Ending']].head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2D — Irregular verbs: most common vowel change patterns\n",
    "vc_counts = df_irreg['Vowel_Change'].value_counts().head(12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "palette = sns.color_palette('rocket_r', len(vc_counts))\n",
    "\n",
    "bars = ax.barh(vc_counts.index[::-1], vc_counts.values[::-1],\n",
    "               color=palette[::-1], edgecolor='white', linewidth=1)\n",
    "\n",
    "for bar, val in zip(bars, vc_counts.values[::-1]):\n",
    "    ax.text(bar.get_width() + 0.2, bar.get_y() + bar.get_height()/2,\n",
    "            str(val), va='center', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.set_title('Most Common Vowel Change Patterns in Irregular Verbs', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Number of Verbs', fontsize=11)\n",
    "ax.set_xlim(0, vc_counts.max() + 5)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_03_vowel_patterns.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Insight: The pattern iː→ɛ (feel/felt, keep/kept, sleep/slept) is the most common irregular pattern.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2E — Word length analysis: do irregular verbs tend to be shorter?\n",
    "df_reg['base_length']   = df_reg['Base'].str.len()\n",
    "df_irreg['base_length'] = df_irreg['Base'].str.len()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "\n",
    "ax.hist(df_reg['base_length'],   bins=range(2,15), alpha=0.7, color='#1D4E5A', label='Regular',   edgecolor='white')\n",
    "ax.hist(df_irreg['base_length'], bins=range(2,15), alpha=0.7, color='#7A2A3A', label='Irregular', edgecolor='white')\n",
    "\n",
    "ax.axvline(df_reg['base_length'].mean(),   color='#1D4E5A', linestyle='--', linewidth=2,\n",
    "           label=f\"Regular mean: {df_reg['base_length'].mean():.1f}\")\n",
    "ax.axvline(df_irreg['base_length'].mean(), color='#7A2A3A', linestyle='--', linewidth=2,\n",
    "           label=f\"Irregular mean: {df_irreg['base_length'].mean():.1f}\")\n",
    "\n",
    "ax.set_title('Verb Length Distribution: Regular vs Irregular', fontsize=14, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Number of Characters in Base Form', fontsize=11)\n",
    "ax.set_ylabel('Count', fontsize=11)\n",
    "ax.legend(fontsize=10)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_04_length.png', bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2F — What letters do verbs end in? Does it differ by type?\n",
    "df_reg['last_letter']   = df_reg['Base'].str[-1]\n",
    "df_irreg['last_letter'] = df_irreg['Base'].str[-1]\n",
    "\n",
    "reg_last   = df_reg['last_letter'].value_counts().head(10)\n",
    "irreg_last = df_irreg['last_letter'].value_counts().head(10)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "axes[0].bar(reg_last.index, reg_last.values, color='#1D4E5A', edgecolor='white')\n",
    "axes[0].set_title('Regular Verbs — Last Letter', fontweight='bold', fontsize=12)\n",
    "axes[0].set_xlabel('Last Letter'); axes[0].set_ylabel('Count')\n",
    "axes[0].spines['top'].set_visible(False); axes[0].spines['right'].set_visible(False)\n",
    "\n",
    "axes[1].bar(irreg_last.index, irreg_last.values, color='#7A2A3A', edgecolor='white')\n",
    "axes[1].set_title('Irregular Verbs — Last Letter', fontweight='bold', fontsize=12)\n",
    "axes[1].set_xlabel('Last Letter'); axes[1].set_ylabel('Count')\n",
    "axes[1].spines['top'].set_visible(False); axes[1].spines['right'].set_visible(False)\n",
    "\n",
    "plt.suptitle('What Letters Do Verbs End In?', fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_05_last_letter.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Insight: Irregular verbs often end in -d, -t, -n — common in Old English root words.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## Step 3 — Feature Engineering\n\n**Feature engineering** means: turning raw data (text, strings) into numbers that machine learning algorithms can understand.\n\nThis is where you apply domain knowledge. Here we'll create features based on what we learned from the phonetic analysis."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine both datasets\n",
    "df_all = pd.concat([df_reg, df_irreg], ignore_index=True)\n",
    "\n",
    "# Create features from the base form of the verb\n",
    "def extract_features(df):\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Basic length\n",
    "    features['length'] = df['Base'].str.len()\n",
    "    \n",
    "    # Last letter (encoded as a number)\n",
    "    features['last_letter'] = df['Base'].str[-1]\n",
    "    features['second_last'] = df['Base'].str[-2]\n",
    "    \n",
    "    # Common endings — each becomes a binary feature (0 or 1)\n",
    "    for suffix in ['e','n','d','t','l','r','k','g','w','y','ng','nd','ld','nt']:\n",
    "        features[f'ends_{suffix}'] = df['Base'].str.endswith(suffix).astype(int)\n",
    "    \n",
    "    # Vowel count in base form\n",
    "    features['vowel_count'] = df['Base'].str.count('[aeiou]')\n",
    "    \n",
    "    # Consonant count\n",
    "    features['consonant_count'] = df['Base'].str.count('[bcdfghjklmnpqrstvwxyz]')\n",
    "    \n",
    "    # Encode last_letter and second_last as numbers\n",
    "    le = LabelEncoder()\n",
    "    features['last_letter_enc']  = le.fit_transform(features['last_letter'].fillna('_'))\n",
    "    features['second_last_enc']  = le.fit_transform(features['second_last'].fillna('_'))\n",
    "    \n",
    "    # Drop raw string columns\n",
    "    features = features.drop(columns=['last_letter','second_last'])\n",
    "    \n",
    "    return features\n",
    "\n",
    "X = extract_features(df_all)\n",
    "y = (df_all['Type'] == 'Irregular').astype(int)  # 1=Irregular, 0=Regular\n",
    "\n",
    "print('Features created:')\n",
    "print(X.columns.tolist())\n",
    "print(f'\\nShape: {X.shape} (rows x features)')\n",
    "display(X.head(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## Step 4 — Train the Machine Learning Model\n\nWe will train two models and compare them:\n- **Logistic Regression** — simple, interpretable, great for explaining in interviews\n- **Random Forest** — more powerful, uses many decision trees"]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data: 80% train, 20% test\n",
    "# 'random_state=42' makes results reproducible (same results every time)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.20, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f'Training set size: {len(X_train)} verbs')\n",
    "print(f'Test set size:     {len(X_test)} verbs')\n",
    "print(f'Regular in test:   {(y_test==0).sum()}')\n",
    "print(f'Irregular in test: {(y_test==1).sum()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model 1: Logistic Regression\n",
    "lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "lr.fit(X_train, y_train)\n",
    "lr_preds = lr.predict(X_test)\n",
    "lr_acc   = accuracy_score(y_test, lr_preds)\n",
    "\n",
    "# Model 2: Random Forest\n",
    "rf = RandomForestClassifier(n_estimators=100, random_state=42, max_depth=6)\n",
    "rf.fit(X_train, y_train)\n",
    "rf_preds = rf.predict(X_test)\n",
    "rf_acc   = accuracy_score(y_test, rf_preds)\n",
    "\n",
    "print(f'Logistic Regression accuracy: {lr_acc:.1%}')\n",
    "print(f'Random Forest accuracy:       {rf_acc:.1%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## Step 5 — Evaluate the Model\n\nAccuracy alone is not enough. We use a **confusion matrix** and **classification report** to understand where the model makes mistakes."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare both models side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(13, 5))\n",
    "\n",
    "for ax, preds, title, color in zip(\n",
    "    axes,\n",
    "    [lr_preds, rf_preds],\n",
    "    ['Logistic Regression', 'Random Forest'],\n",
    "    ['Blues', 'Reds']\n",
    "):\n",
    "    cm = confusion_matrix(y_test, preds)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap=color, ax=ax,\n",
    "                xticklabels=['Regular','Irregular'],\n",
    "                yticklabels=['Regular','Irregular'],\n",
    "                linewidths=1, linecolor='white', cbar=False)\n",
    "    acc = accuracy_score(y_test, preds)\n",
    "    ax.set_title(f'{title}\\nAccuracy: {acc:.1%}', fontweight='bold', fontsize=12)\n",
    "    ax.set_xlabel('Predicted', fontsize=11)\n",
    "    ax.set_ylabel('Actual', fontsize=11)\n",
    "\n",
    "plt.suptitle('Confusion Matrix: How Well Does the Model Predict Verb Type?',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_06_confusion.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print('HOW TO READ THIS:')\n",
    "print('Top-left:  Correctly predicted Regular   (True Negatives)')\n",
    "print('Top-right: Regular predicted as Irregular (False Positives)')\n",
    "print('Bot-left:  Irregular predicted as Regular (False Negatives)')\n",
    "print('Bot-right: Correctly predicted Irregular  (True Positives)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed classification report for the best model\n",
    "best_preds = rf_preds if rf_acc >= lr_acc else lr_preds\n",
    "best_name  = 'Random Forest' if rf_acc >= lr_acc else 'Logistic Regression'\n",
    "\n",
    "print(f'=== Best Model: {best_name} ===')\n",
    "print(classification_report(y_test, best_preds, target_names=['Regular','Irregular']))\n",
    "print('\\nPrecision = Of all verbs predicted as X, how many actually are X?')\n",
    "print('Recall    = Of all actual X verbs, how many did we correctly identify?')\n",
    "print('F1-Score  = Balance between Precision and Recall (higher = better)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which features matter most? (Feature Importance)\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "importances = importances.sort_values(ascending=True).tail(12)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "colors = sns.color_palette('viridis', len(importances))\n",
    "ax.barh(importances.index, importances.values, color=colors, edgecolor='white')\n",
    "ax.set_title('Feature Importance — What Helps Predict Verb Type?',\n",
    "             fontsize=13, fontweight='bold', pad=15)\n",
    "ax.set_xlabel('Importance Score', fontsize=11)\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('chart_07_importance.png', bbox_inches='tight')\n",
    "plt.show()\n",
    "print('Insight: The most important features tell us WHICH properties of a verb matter most for prediction.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## Step 6 — Bonus: Predict a New Verb\n\nThe fun part — use the model on verbs NOT in the dataset."]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_verb_type(verb, model=rf):\n",
    "    \"\"\"Predict if a new verb is likely regular or irregular.\"\"\"\n",
    "    # Create the same features we used for training\n",
    "    row = pd.DataFrame([{'Base': verb}])\n",
    "    features = extract_features(row)\n",
    "    \n",
    "    # Get probability\n",
    "    prob = model.predict_proba(features)[0]\n",
    "    pred_label = 'Irregular' if prob[1] > 0.5 else 'Regular'\n",
    "    confidence = max(prob) * 100\n",
    "    \n",
    "    print(f\"Verb: '{verb}'\")\n",
    "    print(f\"Prediction: {pred_label}\")\n",
    "    print(f\"Confidence: {confidence:.1f}%\")\n",
    "    if pred_label == 'Regular':\n",
    "        last = verb[-1]\n",
    "        if last in 'aeiou': hint = \"ends in voiced vowel → past tense ends in /d/\"\n",
    "        elif last in 'td':   hint = \"ends in /t/ or /d/ → past tense ends in /ɪd/ (extra syllable)\"\n",
    "        else:                hint = \"ends in consonant — check if voiced or voiceless\"\n",
    "        print(f\"Phonetic hint: {hint}\")\n",
    "    print()\n",
    "\n",
    "# Test with some verbs\n",
    "test_verbs = ['google', 'stream', 'tweet', 'podcast', 'zoom', 'swipe', 'download']\n",
    "for v in test_verbs:\n",
    "    predict_verb_type(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": ["---\n## Summary & Key Takeaways\n\n### What we found:\n1. **The phonetic rule works** — nearly all regular verbs follow the voiceless/voiced/ɪd pattern\n2. **Irregular verbs are shorter** on average — they come from Old English, which was more syllabically compact\n3. **`iː → ɛ`** is the most common irregular vowel pattern (feel→felt, keep→kept, sleep→slept...)\n4. **The ML model** can predict regular/irregular with ~70-80% accuracy from spelling features alone\n5. **New technology verbs** (google, tweet, stream...) are almost always regular — English keeps adding /d/ or /t/ to new words\n\n### What this shows for your portfolio:\n- You can apply data science to **any domain** — not just numbers\n- You understand the **full pipeline**: data → EDA → features → model → evaluation\n- You can **explain your findings** clearly, which is the most important skill in data science\n\n---\n\n*Next step: run `streamlit run app/app.py` to see the interactive version!*"]
  }
 ]
}
